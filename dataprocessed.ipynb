{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15523cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Obtaining dependency information for python-docx from https://files.pythonhosted.org/packages/3e/3d/330d9efbdb816d3f60bf2ad92f05e1708e4a1b9abe80461ac3444c83f749/python_docx-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in d:\\users\\70794\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in d:\\users\\70794\\anaconda3\\lib\\site-packages (from python-docx) (4.10.0)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "   ---------------------------------------- 0.0/244.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 92.2/244.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 244.3/244.3 kB 3.7 MB/s eta 0:00:00\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b091a0-96d3-4a98-a6a4-0ce66a542ab5",
   "metadata": {},
   "source": [
    "### 在Nexis上下载的是word格式的文章。\n",
    "### 文章统一的格式是：第一行标题，第二行报纸名，第三行日期。第四行开始是版权信息，一直到Body这个词，之后是新闻正文。正文结束后是Classification。最后的最后每篇文章以End of Document结束。\n",
    "### 所以分割的时候提取第一行为Title，第三行中提取符合日期格式的部分作为日期。从Body开始提取，直到出现classification结束。\n",
    "### 把这些内容提取为三列，title, date, body, 转换成csv。 \n",
    "### 因为下载下来的word分为好多个，所以转化成了好多个csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8e104-7242-48cc-b2d8-0a2e0f62bd4c",
   "metadata": {},
   "source": [
    "一个word中新闻内容的例子：\n",
    "\n",
    "Women Earn High Marks on a Web Test\n",
    "The New York Times\n",
    "July 3, 1997, Thursday, Late Edition - Final\n",
    "\n",
    "\n",
    "Copyright 1997 The New York Times Company\n",
    "Distribution: Business/Financial Desk\n",
    "Section: Section D;; Section D; Page 6; Column 3; Business/Financial Desk; Column 3;\n",
    "Length: 236 words\n",
    "Body\n",
    "\n",
    "\n",
    "Results of a World Wide Web navigating test, released yesterday by the MCI Communications Corporation and the Educational Testing Service, indicated that older women could find their way around the Web better than young men could. 吧啦吧啦....\n",
    "\n",
    "\n",
    "Classification\n",
    "\n",
    "\n",
    "Language: ENGLISH\n",
    "\n",
    "Subject: WOMEN (91%); ACADEMIC TESTING (90%); EDUCATIONAL TESTING SERVICES (78%)\n",
    "\n",
    "Company: VERIZON COMMUNICATIONS INC  (96%);  EDUCATIONAL TESTING SERVICE  (58%); VERIZON COMMUNICATIONS INC  (96%);  EDUCATIONAL TESTING SERVICE  (58%)\n",
    "\n",
    "Ticker: VZC (LSE)  (96%);  VZ (NYSE)  (96%)\n",
    "\n",
    "Industry: NAICS517210 WIRELESS TELECOMMUNICATIONS CARRIERS (EXCEPT SATELLITE)  (96%);  NAICS517110 WIRED TELECOMMUNICATIONS CARRIERS  (96%);  SIC8733 NONCOMMERCIAL RESEARCH ORGANIZATIONS  (58%); INTERNET & WWW (93%); COMPUTER NETWORKS (90%); SEARCH ENGINES (74%)\n",
    "\n",
    "Person: VINTON CERF (73%)\n",
    "\n",
    "Load-Date: July 3, 1997\n",
    "\n",
    "\n",
    "End of Document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9383e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 85_us_1.DOCX -> internet_us_1.csv\n",
      "Processed 85_us_10.DOCX -> internet_us_10.csv\n",
      "Processed 85_us_11.DOCX -> internet_us_11.csv\n",
      "Processed 85_us_12.DOCX -> internet_us_12.csv\n",
      "Processed 85_us_13.DOCX -> internet_us_13.csv\n",
      "Processed 85_us_14.DOCX -> internet_us_14.csv\n",
      "Processed 85_us_15.DOCX -> internet_us_15.csv\n",
      "Processed 85_us_16.DOCX -> internet_us_16.csv\n",
      "Processed 85_us_17.DOCX -> internet_us_17.csv\n",
      "Processed 85_us_2.DOCX -> internet_us_2.csv\n",
      "Processed 85_us_20.DOCX -> internet_us_20.csv\n",
      "Processed 85_us_21.DOCX -> internet_us_21.csv\n",
      "Processed 85_us_22.DOCX -> internet_us_22.csv\n",
      "Processed 85_us_3.DOCX -> internet_us_3.csv\n",
      "Processed 85_us_4.DOCX -> internet_us_4.csv\n",
      "Processed 85_us_5.DOCX -> internet_us_5.csv\n",
      "Processed 85_us_6.DOCX -> internet_us_6.csv\n",
      "Processed 85_us_7.DOCX -> internet_us_7.csv\n",
      "Processed 85_us_8.DOCX -> internet_us_8.csv\n",
      "Processed 85_us_9.DOCX -> internet_us_9.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "\n",
    "# read word docx\n",
    "def read_word_file(file_path):\n",
    "    doc = Document(file_path)\n",
    "    content = []\n",
    "    for para in doc.paragraphs:\n",
    "        content.append(para.text)\n",
    "    return content\n",
    "\n",
    "# extract topic and content\n",
    "def extract_articles(content):\n",
    "    articles = []\n",
    "    current_article = {'title': '', 'date': '', 'body': ''}\n",
    "    in_body = False\n",
    "\n",
    "\n",
    "    # extract date\n",
    "    date_pattern = re.compile(r'^([A-Z][a-z]+\\s\\d{1,2},\\s\\d{4})', re.IGNORECASE)\n",
    "\n",
    "    for line in content:\n",
    "        # topic\n",
    "        if not current_article['title'] and not date_pattern.match(line):\n",
    "            current_article['title'] = line.strip()\n",
    "        \n",
    "        # date\n",
    "        elif date_pattern.match(line):\n",
    "            \n",
    "            current_article['date'] = date_pattern.search(line).group(1).strip()\n",
    "\n",
    "        # content\n",
    "        elif line.strip() == \"Body\":\n",
    "            in_body = True\n",
    "        \n",
    "        # stop at \"Classification\" \n",
    "        elif line.strip() == \"Classification\":\n",
    "            in_body = False\n",
    "        \n",
    "        # \"End of Document\" means the end\n",
    "        elif line.strip() == \"End of Document\":\n",
    "            articles.append(current_article.copy()) \n",
    "            current_article = {'title': '', 'date': '', 'body': ''} \n",
    "        \n",
    "        elif in_body:\n",
    "            current_article['body'] += line.strip() + ' '\n",
    "\n",
    "    return articles\n",
    "\n",
    "# save as csv\n",
    "def save_to_csv(articles, output_file):\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "def main():\n",
    "    input_dir = 'D:/news/'  \n",
    "    file_pattern = re.compile(r'85_us_(\\d+)\\.DOCX', re.IGNORECASE)  # read the word docx\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        match = file_pattern.match(file_name)\n",
    "        if match:\n",
    "            number_suffix = match.group(1)  # docs name as 85_country_number\n",
    "            input_file = os.path.join(input_dir, file_name)\n",
    "            output_file = f'internet_us_{number_suffix}.csv'  # save as csv\n",
    "            \n",
    "            # read and processed the docs\n",
    "            content = read_word_file(input_file)\n",
    "            articles = extract_articles(content)\n",
    "            save_to_csv(articles, output_file)\n",
    "            print(f'Processed {file_name} -> {output_file}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375e89b-a9b3-4235-8142-1e51b9e991e6",
   "metadata": {},
   "source": [
    "### 然后把这些csv合成了同一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75e05802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv are saved in: internet_us.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# combined all the csv\n",
    "def merge_csv_files(file_paths):\n",
    "    \n",
    "    all_data = pd.concat([pd.read_csv(file) for file in file_paths], ignore_index=True)\n",
    "    \n",
    "    all_data.insert(0, 'id', range(1, len(all_data) + 1))\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# save the combined csv\n",
    "def save_merged_csv(merged_data, output_file):\n",
    "    merged_data.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('C:/Users/70794/news/us/*.csv')  \n",
    "    \n",
    "    merged_data = merge_csv_files(file_paths)\n",
    "\n",
    "    output_file = 'internet_us.csv'\n",
    "    save_merged_csv(merged_data, output_file)\n",
    "    print(f\"csv are saved in: {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795213e-25b3-456b-b414-52c17fd52011",
   "metadata": {},
   "source": [
    "### 然后把date拆成了日、月、年，保存到了新的csv。新的csv在新的notebook里处理。\n",
    "### 这个notebook里只有美国的处理步骤。两个报纸的文档命名只有后缀名不一样，中国的没有保存步骤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e728e553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed csv save to: internet_us_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the combined csv\n",
    "def process_csv(file_path, output_file):\n",
    "    # read csv\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # seperate the date column to day, month, year\n",
    "    df[['month_name', 'day', 'year']] = df['date'].str.extract(r'([A-Za-z]+) (\\d{1,2}), (\\d{4})')\n",
    "    \n",
    "    month_mapping = {\n",
    "        'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5, 'june': 6,\n",
    "        'july': 7, 'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12\n",
    "    }\n",
    "    df['month'] = df['month_name'].str.lower().map(month_mapping)\n",
    "    \n",
    "    df.drop(columns=['date', 'month_name'], inplace=True)\n",
    "    \n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"processed csv save to: {output_file}\")\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    input_file = 'C:/Users/70794/news/us/internet_us.csv'  \n",
    "    output_file = 'internet_us_clean.csv' \n",
    "    \n",
    "\n",
    "    process_csv(input_file, output_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0da2f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
